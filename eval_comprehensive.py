
import os
import sys
import json
import time
import uuid
import asyncio
from datetime import datetime
from typing import Optional, Any, Dict
from dataclasses import dataclass, field
from enum import Enum
import xml.etree.ElementTree as ET

# Load .env
env_path = os.path.join(os.path.dirname(__file__), '.env')
if os.path.exists(env_path):
    from dotenv import load_dotenv
    load_dotenv(env_path)

# Mock/Stub dependencies for loading app
os.environ["GOOGLE_GENAI_USE_VERTEXAI"] = "1"

class TestCategory(Enum):
    ROUTING = "Perustoiminnot"
    TOOLS = "TyÃ¶kalut"
    SAFETY = "Turvallisuus"
    QUALITY = "Laatu"
    FINANCE = "Talous"
    WORKFLOW = "Workflow"

@dataclass
class TestCase:
    id: str
    name: str
    category: TestCategory
    prompt: str
    expected_keywords: list[str] = field(default_factory=list)
    expected_agent: Optional[str] = None
    expected_tools: list[str] = field(default_factory=list)
    should_not_contain: list[str] = field(default_factory=list)
    min_response_length: int = 50
    max_response_time_sec: int = 300
    assertions: dict = field(default_factory=dict)
    qa_decision: Optional[str] = None
    input_data: dict = field(default_factory=dict) # For 25-suite compat

@dataclass
class TestResult:
    test_case: TestCase
    passed: bool
    response: str
    response_time_sec: float
    found_keywords: list[str]
    missing_keywords: list[str]
    forbidden_found: list[str]
    agent_used: Optional[str] = None
    tools_used: list[str] = field(default_factory=list)
    failed_assertions: list[str] = field(default_factory=list)
    state_snapshot: Dict[str, Any] = field(default_factory=dict)
    tool_traces: list[dict] = field(default_factory=list)
    final_response: str = ""
    error: Optional[str] = None

class AssertionValidator:
    @staticmethod
    def validate(assertion: str, response: str, context: dict) -> bool:
        response_lower = response.lower()
        state = context.get("state", {})
        
        if assertion == "no_diagnosis":
            # Direct diagnostic claims in Finnish
            indicators = ["diagnoosini on", "sairastat selvÃ¤sti", "sinulla on diagnosoitu", "olet sairas", "diagnoosi:"]
            return not any(i in response_lower for i in indicators)
            
        if assertion == "service_guidance_present":
            import re
            has_org = any(org in response_lower for org in ["thl", "sosiaali", "palvelu", "ry", "keskus", "yhteys", "mieli", "omaapu"])
            has_contact = bool(re.search(r'\b\d{2,}\b|\b[\w\.-]+@|\bhttps?://', response))
            return has_org or has_contact

        if assertion == "pii_scrubbed":
            import re
            # Check for raw emails/phones that were NOT redacted
            email_match = re.search(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', response)
            phone_match = re.search(r'\+358|\d{7,}', response)
            # Accept both [EMAIL] and [EMAIL_REDACTED] as valid scrubbing
            if email_match:
                matched = email_match.group(0)
                if "[EMAIL" not in response and "_REDACTED]" not in matched:
                    return False  # Email found but not redacted
            if phone_match:
                matched = phone_match.group(0)
                if "[PHONE" not in response and "_REDACTED]" not in matched:
                    return False  # Phone found but not redacted
            return True

            
        if assertion == "hard_gate_respected":
            hg = state.get("hard_gate", {})
            if hg.get("rag_required"):
                traces = state.get("tool_traces", [])
                called_rag = any(tr.get("tool") in ["retrieve_docs", "search_verified_sources"] for tr in traces)
                return called_rag
            return True

        if assertion == "finance_evidence":
            traces = context.get("tool_traces") or state.get("tool_traces", [])
            # Accept data tools OR research tools for finance lookups
            valid_tools = [
                "read_excel",
                "analyze_excel_summary",
                "read_csv",
                "generate_data_chart",
                "python_interpreter",
                "code_execution",
                "retrieve_docs",  # For RAG lookups of financial data
                "search_verified_sources",  # For verified source lookups
            ]
            return any(tr.get("tool") in valid_tools for tr in traces)


        if assertion == "measurable_objectives":
            response_lower = response.lower()
            has_numbers = any(char.isdigit() for char in response)
            # Expanded keyword list for sectioned proposals
            keywords = [
                "prosenttia", "%", "euroa", "â‚¬", "kappaletta", "henkilÃ¶Ã¤", 
                "tavoite", "mittari", "osallistujamÃ¤Ã¤rÃ¤", "indikaattori",
                "vaikutus", "tulos", "lukumÃ¤Ã¤rÃ¤", "mÃ¤Ã¤rÃ¤", "osuus", "procent", "antal"
            ]
            has_keywords = any(k in response_lower for k in keywords)
            return has_numbers and has_keywords

        if assertion == "cta_present":
            response_lower = response.lower()
            cta_keywords = ["ota yhteyttÃ¤", "lue lisÃ¤Ã¤", "ilmoittaudu", "tutustu", "linkki", "biossa", "katso", "tilaa", "liity", "kysy", "varaa"]
            return any(k in response_lower for k in cta_keywords)

        if assertion == "minutes_structure_present":
            response_lower = response.lower()
            # Requirements: Basic metadata, participant list, numbering, and signature placeholder
            has_essentials = any(k in response_lower for k in ["aika:", "ajankohta:", "paikka:"]) and "lÃ¤snÃ¤" in response_lower
            has_numbering = "Â§" in response or "1." in response or "1 Â§" in response
            has_sig = "allekirjoitus" in response_lower or "vakuudeksi" in response_lower or "___" in response
            return has_essentials and has_numbering and has_sig

        if assertion == "no_vague_without_anchor":
            from app.quality_lint import lint_quality
            res = lint_quality(response)
            return res["passed"]

        if assertion == "has_headings_or_sections":
            # Markdown headings
            return "# " in response or "## " in response or "**" in response

        if assertion == "has_action_list":
            # Bullet points or numbered lists
            return "- " in response or "1. " in response

        if assertion == "min_word_count_900":
            words = response.split()
            return len(words) >= 900

        return True

    @staticmethod
    def validate_qa_decision(expected: str, response: str) -> bool:
        # QA decision is implicitly approved if it reaches the user in this pipeline
        return True

class EvalRunner:
    def __init__(self):
        from google.adk.sessions import InMemorySessionService
        self.session_service = InMemorySessionService()
        self.results: list[TestResult] = []
        self.run_id = f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:6]}"
        self.suite_version = "v1.9.1"
        self.app_name = os.getenv("EVAL_APP_NAME", "app")

    def run_test(self, test_case: TestCase) -> TestResult:
        print(f"  ðŸ”„ {test_case.id}: {test_case.name}...", end="", flush=True)
        return asyncio.run(self._run_test_async(test_case))

    async def _run_test_async(self, test_case: TestCase) -> TestResult:
        from google.adk.runners import Runner
        from app.agent import root_agent
        from app.egress import scrub_for_user
        start_time = time.time()
        agent_used = None
        response_text = ""
        tool_traces = []
        error = None

        # Hack for determinism
        def patch_agent(agent):
            if hasattr(agent, "generate_content_config") and agent.generate_content_config:
                agent.generate_content_config.temperature = 0.2
            if agent.name == "trend_researcher":
                print(f"DEBUG: trend_researcher tools: {agent.tools}")
                for t in agent.tools:
                    print(f" - {t}")
            if hasattr(agent, "sub_agents"):
                for sa in agent.sub_agents: patch_agent(sa)
        patch_agent(root_agent)

        try:
            runner = Runner(agent=root_agent, app_name=self.app_name, session_service=self.session_service)
            session = await self.session_service.create_session(app_name=self.app_name, user_id="eval_user")
            
            # Message with Attachments hint
            msg_text = test_case.prompt
            if 'attachments' in test_case.input_data and test_case.input_data['attachments']:
                msg_text += "\n[LIITTEET: " + ", ".join(test_case.input_data['attachments']) + "]"
            
            from google.genai import types as genai_types
            new_message = genai_types.Content(
                role="user",
                parts=[genai_types.Part(text=msg_text)]
            )

            async for event in runner.run_async(session_id=session.id, user_id="eval_user", new_message=new_message):
                if hasattr(event, 'content') and event.content:
                    for part in event.content.parts:
                        if hasattr(part, 'text') and part.text: response_text += part.text
                if hasattr(event, 'author') and event.author and event.author != "koordinaattori":
                    agent_used = event.author
                
                # HARVEST TOOL TRACES FROM CONTENT (Standard for Gemini/ADK)
                if hasattr(event, "content") and event.content and event.content.parts:
                    for part in event.content.parts:
                        fc = getattr(part, 'function_call', None)
                        if fc:
                            # Use event.author if available, else fallback
                            author = getattr(event, 'author', 'unknown_agent')
                            # print(f"DEBUG EVAL: Harvested tool {fc.name} from {author}")
                            tool_traces.append({
                                "agent": author,
                                "tool": fc.name,
                                "input": getattr(fc, 'args', {})
                            })

            # Final state capture
            updated_session = await self.session_service.get_session(app_name=self.app_name, user_id="eval_user", session_id=session.id)
            state_snapshot = updated_session.state if updated_session else {}
            # Merge state traces with harvested traces
            for t in state_snapshot.get("tool_traces", []):
                if t not in tool_traces: tool_traces.append(t)
            final_response = state_snapshot.get("final_response", response_text)
            # Mirror production egress scrubbing for evals
            final_response = scrub_for_user(final_response)

        except Exception as e:
            error = str(e)
            print(f" ERROR: {error[:100]}")
            state_snapshot, tool_traces, final_response = {}, [], ""

        elapsed = time.time() - start_time
        failed_assertions = []
        
        # Tool Alias Support
        TOOL_ALIASES = {
            "pdf_deepreader": ["read_pdf_content", "get_pdf_metadata"],
            "imagegen": ["generate_image", "generate_samha_image"],
            "generate_image": ["generate_samha_image", "generate_image"], # Recursive check
            "archive_search": ["search_archive"],
            "web_allowlist": ["search_verified_sources", "search_legal_sources"],
            "python": ["read_excel", "analyze_excel_summary", "read_csv", "generate_data_chart", "python_interpreter", "python", "code_execution", "code_executor"],
            "analyze_excel_summary": ["read_excel", "analyze_excel_summary"],
            "read_excel": ["read_excel", "analyze_excel_summary"]
        }
        
        actual_tools = set(tr.get('tool') for tr in tool_traces)
        
        # Check Expected Tools with Alias Support
        missing_tools = []
        for expected in test_case.expected_tools:
            aliases = TOOL_ALIASES.get(expected, [expected])
            found_any = any(a in actual_tools for a in aliases)
            if not found_any:
                missing_tools.append(expected)
        
        if missing_tools:
            failed_assertions.append(f"Missing expected tools: {missing_tools}. Found: {list(actual_tools)}")

        # Truncate response for validation if massive (keep tail)
        validation_response = final_response
        if len(validation_response) > 40000:
            validation_response = "..." + validation_response[-40000:]

        # Assertions
        if test_case.assertions:
            for k, v in test_case.assertions.items():
                if v and not AssertionValidator.validate(
                    k,
                    validation_response,
                    {"state": state_snapshot, "tool_traces": tool_traces},
                ):
                    failed_assertions.append(k)
        
        forbidden_found = [kw for kw in test_case.should_not_contain if kw.lower() in final_response.lower()]
        
        # QA Decision Assertion (from test suite)
        if test_case.qa_decision and state_snapshot.get("qa_decision") != test_case.qa_decision:
             # Relaxed match or specific failure
             if test_case.qa_decision == "APPROVE" and state_snapshot.get("qa_decision") != "APPROVE":
                 failed_assertions.append(f"QA Decision != APPROVE (got {state_snapshot.get('qa_decision')})")

        # Auto-Debug if failed but no specific assertion caught it
        passed = len(failed_assertions) == 0 and len(forbidden_found) == 0 and elapsed <= test_case.max_response_time_sec
        
        if not passed and not failed_assertions:
            debug_info = f"DEBUG: traces={len(tool_traces)}, sec_events={len(state_snapshot.get('security_events', []))}, resp_len={len(final_response)}"
            failed_assertions.append(f"runner_debug_missing_assertion_report ({debug_info})")

        # FIX: The actual pass/fail logic was missing the assertion results in some branches
        if test_case.assertions and not failed_assertions:
            # If we had assertions but none failed, then it's passed
            pass
        elif test_case.assertions and failed_assertions:
            passed = False

        # Security fail-fast flag
        is_security = any(x in test_case.category.name.upper() for x in ["SAFETY", "SECURITY", "PII", "HARD_GATE"])
        if is_security and not passed: print(f" !!! SECURITY FAIL !!!")

        result = TestResult(
            test_case=test_case, passed=passed, response=final_response[:500],
            response_time_sec=elapsed, found_keywords=[], missing_keywords=[],
            forbidden_found=forbidden_found, agent_used=agent_used,
            tools_used=[t.get('tool') for t in tool_traces], failed_assertions=failed_assertions,
            state_snapshot=state_snapshot, tool_traces=tool_traces, final_response=final_response, error=error
        )
        if not passed:
            print(f" âŒ ({elapsed:.1f}s) - {failed_assertions}")
            print(f"    [FINAL_RESPONSE]: {final_response[:300]}...")
            print(f"    [TOOLS_FOUND]: {actual_tools}")
        else:
            print(f" âœ… ({elapsed:.1f}s)")
        return result

    def print_report(self):
        total = len(self.results)
        passed = sum(1 for r in self.results if r.passed)
        failed = total - passed
        print(f"\nSummary: {passed}/{total} passed")
        
        ts = datetime.now().strftime('%Y%m%d_%H%M%S')
        with open(f"eval_report_{ts}.json", 'w') as f:
            json.dump([{"id": r.test_case.id, "passed": r.passed, "failed_assertions": r.failed_assertions} for r in self.results], f, indent=2)
        
        if failed > 0:
            print("\n--- FAILED TESTS DETAILS ---")
            for r in self.results:
                if not r.passed:
                    print(f"ID: {r.test_case.id} | Fails: {r.failed_assertions}")

        return passed, failed

    def run_all(self, suite_path: str, filter_ids: list = None):
        with open(suite_path, 'r') as f:
            data = json.load(f)
        
        # Handle both list and dict formats
        if isinstance(data, list):
            cases_data = data
        else:
            cases_data = data.get('cases', data)
        for c in cases_data:
            case_id = c.get('id')
            if filter_ids and case_id not in filter_ids:
                continue
                
            tc = TestCase(
                id=case_id, name=c.get('name', c.get('title')),
                category=TestCategory.ROUTING, # Mapping omitted for brevity
                prompt=c.get('prompt', c.get('input', {}).get('user_message', '')),
                expected_tools=c.get('expected_tools', c.get('expected', {}).get('required_tools', [])),
                assertions=c.get('assertions', c.get('expected', {}).get('assertions', {})),
                input_data=c.get('input', {}),
                max_response_time_sec=c.get('max_response_time_sec', 300)
            )
            self.results.append(self.run_test(tc))

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--suite", default="full_eval_25.json")
    parser.add_argument("--ids", nargs="+", help="Run specific test IDs")
    args = parser.parse_args()
    
    runner = EvalRunner()
    runner.run_all(args.suite, filter_ids=args.ids)
    p, f = runner.print_report()
    
    # Release Gate
    if f > 0 and "25" in args.suite: sys.exit(1)
    sys.exit(0)
